<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="HyperHuman Project Page">

  <title>HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion</title>

  <link href="./static/css/bootstrap.min.css" rel="stylesheet">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link href="./static/css/font.css" rel="stylesheet" type="text/css">
  <link href="./static/css/style.css" rel="stylesheet" type="text/css">

</head>
<!-- === Header Ends === -->


<body>


<!-- === Home Section Starts === -->
<div class="section">
  <!-- === Title Starts === -->
  <div class="header">
    <!-- <div class="logo">
      <a href="https://mmlab.ie.cuhk.edu.hk/" target="_blank"><img src="./common/cuhk.jfif"></a>
    </div> -->
    <div class="title", style="font-size: 24pt; padding-top: 10pt;">  <!-- Set padding as 10 if title is with two lines. -->
      HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion
		<!-- <br>
		<font color="grey" size="4">arXiv Preprint.</font> -->
    </div>
  </div>
  <!-- === Title Ends === -->
  <div class="author">
    <a href="https://alvinliu0.github.io/" target="_blank">Xian Liu</a><sup>1,2</sup>,&nbsp;
    <a href="https://alanspike.github.io/" target="_blank">Jian Ren</a><sup>1</sup>,&nbsp;
    <a href="https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/" target="_blank">Aliaksandr Siarohin</a><sup>1</sup>,&nbsp;
    <a href="https://universome.github.io/" target="_blank">Ivan Skorokhodov</a><sup>1</sup>,&nbsp;
    <a href="https://scholar.google.com/citations?user=XUj8koUAAAAJ&hl=en" target="_blank">Yanyu Li</a><sup>1</sup>,&nbsp;
    <br>
    <a href="http://dahua.site/" target="_blank">Dahua Lin</a><sup>2</sup>,&nbsp;
    <a href="https://xh-liu.github.io/" target="_blank">Xihui Liu</a><sup>3</sup>,&nbsp;
    <a href="https://liuziwei7.github.io/" target="_blank">Ziwei Liu</a><sup>4</sup>,&nbsp;
    <a href="http://www.stulyakov.com/" target="_blank">Sergey Tulyakov</a><sup>1</sup>
  </div>
  <div class="institution">
    <sup>1</sup>Snap Inc.&nbsp;&nbsp;&nbsp;
	<sup>2</sup>CUHK&nbsp;&nbsp;&nbsp;
	<sup>3</sup>HKU&nbsp;&nbsp;&nbsp;
	<sup>4</sup>NTU
  </div>

  <div class="column has-text-centered">
    <div class="publication-links">
      <!-- PDF Link. -->
      <span class="link-block">
        <a href="./contents/hyperhuman.pdf" class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fas fa-file-pdf"></i>
          </span>
          <span>Paper</span>
        </a>
      </span>
      <span class="link-block">
        <a href="https://arxiv.org/abs/xxxxxx" class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="ai ai-arxiv"></i>
          </span>
          <span>arXiv</span>
        </a>
      </span>
      <!-- Video Link. -->
      <!-- <span class="link-block">
        <a href="https://www.youtube.com/watch?v=Fvenkw7yeok"
          class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fab fa-youtube"></i>
          </span>
          <span>Demo</span>
        </a>
      </span> -->
      <!-- Code Link. -->
      <span class="link-block">
        <a href="https://github.com/snap-research/HyperHuman"
          class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fab fa-github"></i>
          </span>
          <span>Code</span>
        </a>
      </span>
      <!-- Dataset Link.
      <span class="link-block">
        <a href="./#BibTeX" class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fa fa-quote-left"></i>
          </span>
          <span>Cite</span>
        </a> -->
    </div>

  </div>
  <!-- <div class="link">
    <a href="https://arxiv.org/pdf/2212.02350.pdf" target="_blank">[Paper]</a>&nbsp;
    <a href="https://github.com/alvinliu0/ANGIE" target="_blank">[Code]</a>&nbsp;
    <a href="https://github.com/alvinliu0/ANGIE" target="_blank">[Dataset]</a>
  </div> -->
  <div class="teaser">
    <img src="./content/teaser.png">
  </div>
</div>
<!-- === Home Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Abstract</div>
  <div class="body">
    Despite significant advances in large-scale text-to-image models, achieving hyper-realistic human image generation remains a desirable yet unsolved task.
    Existing models like Stable Diffusion and DALLÂ·E 2 tend to generate human images with incoherent parts or unnatural poses. 
    To tackle these challenges, our key insight is that human image is inherently structural over multiple granularities, from the coarse-level body skeleton to the fine-grained spatial geometry. 
    Therefore, capturing such correlations between the explicit appearance and latent structure in one model is essential to generate coherent and natural human images.
    To this end, we propose a unified framework, <b>HyperHuman</b>, that generates in-the-wild human images of high realism and diverse layouts. 
    Specifically, <b>1)</b> we first build a large-scale human-centric dataset, named <i>HumanVerse</i>, which consists of 340M images with comprehensive annotations like human pose, depth, and surface-normal. 
    <b>2)</b> Next, we propose a <i>Latent Structural Diffusion Model</i> that simultaneously denoises the depth and surface-normal along with the synthesized RGB image. Our model enforces the joint learning of image appearance, spatial relationship, and geometry in a unified network, where each branch in the model complements to each other with both structural awareness and textural richness. 
    <b>3)</b> Finally, to further boost the visual quality, we propose a <i>Structure-Guided Refiner</i> to compose the predicted conditions for more detailed generation of higher resolution. 
    Extensive experiments demonstrate that our framework yields the state-of-the-art performance, generating hyper-realistic human images under diverse scenarios. 
  </div>
</div>
<!-- === Overview Section Ends === -->

<div class="section">
  <div class="title">Framework Overview</div>
  <div class="body">

    <!-- Adjust the number of rows and columns (EVERY project differs). -->
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./content/framework.png" width="90%"></td>
      </tr>
    </table>
    <b>Overview of HyperHuman Framework.</b> In <i>Latent Structural Diffusion Model</i> (<span style="color: rgb(230, 128, 230); white-space: nowrap">purple</span>), the image <b>x</b>, depth <b>d</b>, and surface-normal <b>n</b> are jointly denoised conditioning on caption <b>c</b> and pose skeleton <b>p</b>. In <i>Structure-Guided Refiner</i> (<span style="color: rgb(46, 118, 181); white-space: nowrap">blue</span>), we compose the predicted conditions for higher-resolution generation. Note that the grey images refer to randomly dropout conditions for more robust training.
  </div>
</div>

<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Quantitative Results</div>
  <div class="body">

    <!-- Adjust the number of rows and columns (EVERY project differs). -->
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./content/quantitative1.png" width="90%"></td>
      </tr>
    </table>
    <b>Zero-Shot Evaluation on MS-COCO 2014 Validation Human.</b> We compare our model with recent SOTA general T2I models (Stable Diffusion v1.5, v2.0, v2.1; SDXL; DeepFloyd-IF) and controllable methods (ControlNet; T2I-Adapter; HumanSD). Note that SDXL generates artistic style in 512x512, and IF only creates fixed-size images, we first generate 1024x1024 results, then resize back to 512x512 for these two methods. We bold the <b>best</b> and underline the <u>second</u> results for clarity. Our improvements over the second method are shown in <span style="color: rgb(255, 0, 0); white-space: nowrap">red</span>.
  
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./content/quantitative2.png" width="90%"></td>
      </tr>
    </table>
    <b>Evaluation Curves on MS-COCO 2014 Validation Human Subset.</b> We show FID-CLIP (<i>left</i>) and FID<sub>CLIP</sub>-CLIP (<i>right</i>) curves with CFG scale ranging from 4.0 to 20.0 for all methods.
  
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./content/quantitative3.png" width="90%"></td>
      </tr>
    </table>
    <b>User Preference Comparisons.</b> We report the ratio of users prefer our model to baselines.
  </div>
</div>
<!-- === Result Section Ends === -->

<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">More Qualitative Comparisons (1024x1024)</div>
  <div class="body">

    <!-- Adjust the number of rows and columns (EVERY project differs). -->
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <div class="container" width=1000px>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-clevr" style="text-align: center;">
            <img src="./content/additional_comparison1.png" width="90%">
          </div>
          <div class="item item-clevr" style="text-align: center;">
            <img src="./content/additional_comparison2.png" width="90%">
          </div>
          <div class="item item-clevr" style="text-align: center;">
            <img src="./content/additional_comparison3.png" width="90%">
          </div>
          <div class="item item-clevr" style="text-align: center;">
            <img src="./content/additional_comparison4.png" width="90%">
          </div>
        </div>
      </div>
    </table>
  
  </div>
</div>
<!-- === Result Section Ends === -->

<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">More Qualitative Results (1024x1024)</div>
  <div class="body">

    <!-- Adjust the number of rows and columns (EVERY project differs). -->
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <div class="container" width=1000px>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-clevr" style="text-align: center;">
            <img src="./content/our_more1.png" width="90%">
          </div>
          <div class="item item-clevr" style="text-align: center;">
            <img src="./content/our_more2.png" width="90%">
          </div>
          <div class="item item-clevr" style="text-align: center;">
            <img src="./content/our_more3.png" width="90%">
          </div>
        </div>
      </div>

      <!-- <tr>
        <td><img src="./content/our_more1.png" width="90%"></td>
      </tr> -->
    </table>
  
  </div>
</div>
<!-- === Result Section Ends === -->

<!-- === Reference Section Starts === -->
<div class="section">
  <div class="bibtex">BibTeX</div>
<pre>
@article{liu2023hyperhuman,
    title={HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion},
    author={Liu, Xian and Ren, Jian and Siarohin, Aliaksandr and Skorokhodov, Ivan and Li, Yanyu and Lin, Dahua and Liu, Xihui and Liu, Ziwei and Tulyakov, Sergey},
    journal={arXiv preprint arXiv:xxxxxx},
    year={2023}
}
</pre>


</body>
</html>